{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "317ebbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this code use connection to a dataBase\n",
    "## it conatin a GUI interface\n",
    "\n",
    "## if you face any problem running this code try the last call below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the DataBase contain table named Qustions with \"id\",\"translated_qustion\",\"question\",\"answer\"\n",
    "## the translated qustion is found by translate(question) then stored in the DataBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91523cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\programdata\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbe33ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in d:\\programdata\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4978370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d3/1d/a257913c89572de61316461db91867f87519146e58132cdeace3d9ffbe1f/torch-2.3.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached torch-2.3.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Obtaining dependency information for mkl<=2021.4.0,>=2021.1.1 from https://files.pythonhosted.org/packages/fe/1c/5f6dbf18e8b73e0a5472466f0ea8d48ce9efae39bd2ff38cebf8dce61259/mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Obtaining dependency information for intel-openmp==2021.* from https://files.pythonhosted.org/packages/6f/21/b590c0cc3888b24f2ac9898c41d852d7454a1695fbad34bee85dba6dc408/intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Obtaining dependency information for tbb==2021.* from https://files.pythonhosted.org/packages/f1/24/500811330b3b070e5995c3275181dbcd00c06cef26c6ebfe6ee1ca9b6223/tbb-2021.13.0-py3-none-win_amd64.whl.metadata\n",
      "  Using cached tbb-2021.13.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.3.1-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.13.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: tbb, intel-openmp, typing-extensions, mkl, torch\n",
      "  Attempting uninstall: tbb\n",
      "    Found existing installation: TBB 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d655958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import pyodbc\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import font as tkfont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the translate model and tokenizer\n",
    "tokenizer_translate = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n",
    "model_translate = TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eacb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  # Encode the text\n",
    "  input_ids = tokenizer_translate.encode(sentence, return_tensors=\"tf\")\n",
    "\n",
    "  # Generate the translated text\n",
    "  outputs = model_translate.generate(input_ids)\n",
    "  translated_text = tokenizer_translate.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "  return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4297ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bert pre-trained model and tokenizer\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "model_bert = AutoModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ecf910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_similarity(sentence1 , sentence2):\n",
    "\n",
    "#   translated_sentence1 = translate(sentence1)\n",
    "#   translated_sentence2 = translate(sentence2)\n",
    "  translated_sentence1 = sentence1\n",
    "  translated_sentence2 = sentence2\n",
    "\n",
    "  # Encode sentences and compute embeddings\n",
    "  inputs1 = tokenizer_bert(translated_sentence1, return_tensors='pt')\n",
    "  inputs2 = tokenizer_bert(translated_sentence2, return_tensors='pt')\n",
    "\n",
    "  with torch.no_grad():\n",
    "      embeddings1 = model_bert(**inputs1).last_hidden_state.mean(dim=1)\n",
    "      embeddings2 = model_bert(**inputs2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "  # Compute cosine similarity\n",
    "  similarity_bert = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "  return similarity_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fceefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translated_qustions():\n",
    "    #connect to the database\n",
    "    server = 'server name' \n",
    "    database = 'database naem'\n",
    "    cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "    \n",
    "    cursor = cnxn.cursor()\n",
    "    #load the english qustions\n",
    "    cursor.execute('SELECT translated_question, id from Questions')\n",
    "\n",
    "    pairs_question_id = []\n",
    "    for row in cursor:\n",
    "        pairs_question_id.append((row.translated_question, row.id))\n",
    "        \n",
    "    cnxn.close()\n",
    "    \n",
    "    return pairs_question_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2407ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_sentence(user_sentence):\n",
    "    \n",
    "    translated_user_sentence = translate(user_sentece)\n",
    "    \n",
    "    # Get the list of pairs from the get_translated_questions function\n",
    "    translated_questions = get_translated_questions()\n",
    "\n",
    "    similarity_scores = []\n",
    "    for question, id in translated_questions:\n",
    "        # Calculate the similarity score between the user sentence and the current question\n",
    "        similarity = bert_similarity(translated_user_sentence, question)\n",
    "        similarity_scores.append((similarity, question, id))\n",
    "\n",
    "    # Sort the list in descending order of similarity score\n",
    "    similarity_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Return the top 3 most similar questions\n",
    "    return similarity_scores[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "384bd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'server name' \n",
    "database = 'database name'\n",
    "cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';Trusted_Connection=yes;')\n",
    "\n",
    "cursor = cnxn.cursor()\n",
    "\n",
    "def submit():\n",
    "    sentence = entry.get()\n",
    "    results = get_similar_sentence(sentence)\n",
    "    result_text.delete(1.0, tk.END)\n",
    "    question_buttons.clear()\n",
    "    for result in results:\n",
    "        cursor.execute('SELECT question, answer FROM Questions WHERE id = ?', (result[2],))\n",
    "        question, answer = cursor.fetchone()\n",
    "        \n",
    "\n",
    "        start_index = result_text.index(tk.END)\n",
    "        result_text.insert(tk.END, f\"Question: {question}\\n\", f\"question_tag_{result[2]}\")\n",
    "        \n",
    "\n",
    "        button_frame = tk.Frame(root, bg='lightblue')\n",
    "        show_answer_button = tk.Button(button_frame, text=\"Show Answer\", command=lambda q_id=result[2], btn_frame=button_frame: show_answer(q_id, btn_frame), bg='darkblue', fg='white', font=('Helvetica', 10, 'bold'))\n",
    "        show_answer_button.pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "\n",
    "        result_text.window_create(tk.END, window=button_frame)\n",
    "        result_text.insert(tk.END, \"\\n\\n\") \n",
    "        \n",
    "\n",
    "        question_buttons[result[2]] = False\n",
    "\n",
    "def show_answer(q_id, button_frame):\n",
    "    if question_buttons[q_id]:\n",
    "        return\n",
    "    \n",
    "    cursor.execute('SELECT question, answer FROM Questions WHERE id = ?', (q_id,))\n",
    "    question, answer = cursor.fetchone()\n",
    "    \n",
    "\n",
    "    question_index = result_text.search(f\"Question: {question}\", \"1.0\", tk.END)\n",
    "    if question_index:\n",
    "        result_text.insert(f\"{question_index} lineend\", f\"\\nAnswer: {answer}\\n\")\n",
    "        question_buttons[q_id] = True\n",
    "    \n",
    "\n",
    "    button_frame.destroy()\n",
    "\n",
    "root = tk.Tk()\n",
    "root.geometry('400x500')\n",
    "root.configure(bg='lightblue')\n",
    "\n",
    "# Set a nice font for the labels and text\n",
    "title_font = tkfont.Font(family='Helvetica', size=16, weight='bold')\n",
    "text_font = tkfont.Font(family='Helvetica', size=14)\n",
    "button_font = tkfont.Font(family='Helvetica', size=12, weight='bold')\n",
    "\n",
    "label = tk.Label(root, text=\"Enter a sentence:\", bg='lightblue', fg='darkblue', font=title_font, pady=10)\n",
    "label.pack()\n",
    "\n",
    "entry = tk.Entry(root, font=text_font, width=40, bd=2, relief=tk.GROOVE)\n",
    "entry.pack(pady=5)\n",
    "\n",
    "submit_button = tk.Button(root, text=\"Submit\", command=submit, bg='darkblue', fg='white', font=button_font, pady=5)\n",
    "submit_button.pack(pady=10)\n",
    "\n",
    "result_text = tk.Text(root, bg='lightblue', fg='darkblue', font=text_font, wrap=tk.WORD, relief=tk.GROOVE, bd=2)\n",
    "result_text.pack(expand=True, fill=tk.BOTH, padx=10, pady=10)\n",
    "\n",
    "question_buttons = {}\n",
    "\n",
    "root.mainloop()\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d34445",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First, let's create the SQLite database from the CSV file\n",
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file into a pandas DataFrame\n",
    "# df = pd.read_csv('data.csv')\n",
    "\n",
    "# # Connect to SQLite database (or create it if it doesn't exist)\n",
    "# conn = sqlite3.connect('database.db')\n",
    "\n",
    "# # Write the DataFrame to the SQLite database\n",
    "# df.to_sql('Questions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# conn.close()\n",
    "\n",
    "# # Now, let's define the functions to compute BERT similarity and fetch questions from the database\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import torch\n",
    "\n",
    "# # Load BERT pre-trained model and tokenizer\n",
    "# tokenizer_bert = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "# model_bert = AutoModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# #load the translate model and tokenizer\n",
    "# tokenizer_translate = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n",
    "# model_translate = TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n",
    "\n",
    "# def translate(sentence):\n",
    "#     # Encode the text\n",
    "#     input_ids = tokenizer_translate.encode(sentence, return_tensors=\"tf\")\n",
    "\n",
    "#     # Generate the translated text\n",
    "#     outputs = model_translate.generate(input_ids)\n",
    "#     translated_text = tokenizer_translate.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     return translated_text\n",
    "\n",
    "# def bert_similarity(sentence1, sentence2):\n",
    "#     # Encode sentences and compute embeddings\n",
    "#     inputs1 = tokenizer_bert(sentence1, return_tensors='pt')\n",
    "#     inputs2 = tokenizer_bert(sentence2, return_tensors='pt')\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         embeddings1 = model_bert(**inputs1).last_hidden_state.mean(dim=1)\n",
    "#         embeddings2 = model_bert(**inputs2).last_hidden_state.mean(dim=1)\n",
    "\n",
    "#     # Compute cosine similarity\n",
    "#     similarity_bert = cosine_similarity(embeddings1.numpy(), embeddings2.numpy())\n",
    "\n",
    "#     return similarity_bert.item()\n",
    "\n",
    "# def get_questions():\n",
    "#     conn = sqlite3.connect('my_database.db')\n",
    "#     cursor = conn.cursor()\n",
    "#     cursor.execute('SELECT translated_question, id FROM Questions')\n",
    "#     pairs_question_id = [(row[0], row[1]) for row in cursor.fetchall()]\n",
    "#     conn.close()\n",
    "#     return pairs_question_id\n",
    "\n",
    "# def get_similar_sentence(user_sentence):\n",
    "#     questions = get_questions()\n",
    "#     similarity_scores = [(bert_similarity(user_sentence, question), question, id) for question, id in questions]\n",
    "#     similarity_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "#     return similarity_scores[:3]\n",
    "\n",
    "# def main():\n",
    "#     user_sentence_arabic = input(\"Enter a sentence: \")\n",
    "#     user_sentence = translate(user_sentence_arabic)\n",
    "#     results = get_similar_sentence(user_sentence)\n",
    "\n",
    "#     for result in results:\n",
    "#         print(f\"Question: {result[1]} (Similarity: {result[0]:.2f})\")\n",
    "#         conn = sqlite3.connect('my_database.db')\n",
    "#         cursor = conn.cursor()\n",
    "#         cursor.execute('SELECT question, answers FROM Questions WHERE id = ?', (result[2],))\n",
    "#         row = cursor.fetchone()\n",
    "#         question, answer = row[0], row[1]\n",
    "#         print(f\"Original Question: {question}\")\n",
    "#         print(f\"Answer: {answer}\\n\")\n",
    "#         conn.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
